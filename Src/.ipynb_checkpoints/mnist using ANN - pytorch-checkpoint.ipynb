{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d376044a",
   "metadata": {
    "papermill": {
     "duration": 0.022393,
     "end_time": "2021-10-27T20:35:37.167615",
     "exception": false,
     "start_time": "2021-10-27T20:35:37.145222",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## INTRODUCTION\n",
    "- Itâ€™s a Python based scientific computing package targeted at two sets of audiences:\n",
    "    - A replacement for NumPy to use the power of GPUs\n",
    "    - Deep learning research platform that provides maximum flexibility and speed\n",
    "- pros: \n",
    "    - Interactively debugging PyTorch. Many users who have used both frameworks would argue that makes pytorch significantly easier to debug and visualize.\n",
    "    - Clean support for dynamic graphs\n",
    "    - Organizational backing from Facebook\n",
    "    - Blend of high level and low level APIs\n",
    "- cons:\n",
    "    - Much less mature than alternatives\n",
    "    - Limited references / resources outside of the official documentation\n",
    "- I accept you know neural network basics. If you do not know, you should learn the basics first of all.  Because I will not explain neural network concepts detailed, I only explain how to use pytorch for neural network\n",
    "<br>\n",
    "<br> **Content:**\n",
    "0. [Introduction to ANN](#0)\n",
    "1. [Import necessary Libraries](#1)\n",
    "1. [Prepare Dataset](#2)\n",
    "1. [Create ANN Model](#3)\n",
    "1. [ANN model training](#4)\n",
    "1. [visualization loss](#5)\n",
    "1. [visualization accuracy](#6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b850c926",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.039243,
     "end_time": "2021-10-27T20:35:37.220424",
     "exception": false,
     "start_time": "2021-10-27T20:35:37.181181",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bc0ea",
   "metadata": {
    "papermill": {
     "duration": 0.013875,
     "end_time": "2021-10-27T20:35:37.246857",
     "exception": false,
     "start_time": "2021-10-27T20:35:37.232982",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"0\"></a> <br>\n",
    "### Artificial Neural Network (ANN)\n",
    "- Logistic regression is good at classification but when complexity(non linearity) increases, the accuracy of model decreases.\n",
    "- Therefore, we need to increase complexity of model.\n",
    "- In order to increase complexity of model, we need to add more non linear functions as hidden layer. \n",
    "- What we expect from artificial neural network is that when complexity increases, we use more hidden layers and our model can adapt better. As a result accuracy increase.\n",
    "- **Steps of ANN:**\n",
    "    1. Import Libraries\n",
    "\n",
    "    1. Prepare Dataset\n",
    "\n",
    "    1. Create ANN Model\n",
    "        - We add 3 hidden layers.\n",
    "        - We use ReLU, Tanh and ELU activation functions for diversity.\n",
    "    1. Instantiate Model Class\n",
    "        - input_dim = 28*28 # size of image px*px\n",
    "        - output_dim = 10  # labels 0,1,2,3,4,5,6,7,8,9\n",
    "        - Hidden layer dimension is 150. I only choose it as 150 there is no reason. Actually hidden layer dimension is hyperparameter and it should be chosen and tuned. You can try different values for hidden layer dimension and observe the results.\n",
    "        - create model\n",
    "    1. Instantiate Loss\n",
    "        - Cross entropy loss\n",
    "        - It also has softmax(logistic function) in it.\n",
    "    1. Instantiate Optimizer\n",
    "        - SGD Optimizer\n",
    "    1. Traning the Model\n",
    "    1. Prediction\n",
    "- As a result, as you can see from plot, while loss decreasing, accuracy is increasing and our model is learning(training). \n",
    "- Thanks to hidden layers model learnt better and accuracy(almost 95%) is better than accuracy of logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c36f427",
   "metadata": {
    "papermill": {
     "duration": 0.012046,
     "end_time": "2021-10-27T20:35:37.272697",
     "exception": false,
     "start_time": "2021-10-27T20:35:37.260651",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a> <br>\n",
    "### Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d819cbf",
   "metadata": {
    "papermill": {
     "duration": 2.066154,
     "end_time": "2021-10-27T20:35:39.351131",
     "exception": false,
     "start_time": "2021-10-27T20:35:37.284977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\model_selection\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LearningCurveDisplay, ValidationCurveDisplay\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_search\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GridSearchCV, ParameterGrid, ParameterSampler, RandomizedSearchCV\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_split\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      6\u001b[0m     BaseCrossValidator,\n\u001b[0;32m      7\u001b[0m     BaseShuffleSplit,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     24\u001b[0m     train_test_split,\n\u001b[0;32m     25\u001b[0m )\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\model_selection\\_plot.py:7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_matplotlib_support\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_plotting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _interval_max_min_ratio, _validate_score_name\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m learning_curve, validation_curve\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01m_BaseCurveDisplay\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_plot_curve\u001b[39m(\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m     13\u001b[0m         x_data,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m         errorbar_kw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     24\u001b[0m     ):\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone, is_classifier\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FitFailedWarning, UnsetMetadataPassedError\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_scoring, get_scorer_names\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_scorer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _check_multimetric_scoring, _MultimetricScorer\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\metrics\\__init__.py:7\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;124;03mThe :mod:`sklearn.metrics` module includes score functions, performance metrics\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03mand pairwise metrics and distance computations.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cluster\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_classification\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     accuracy_score,\n\u001b[0;32m     10\u001b[0m     balanced_accuracy_score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     27\u001b[0m     zero_one_loss,\n\u001b[0;32m     28\u001b[0m )\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DistanceMetric\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\metrics\\cluster\\__init__.py:25\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bicluster\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m consensus_score\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_supervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     10\u001b[0m     adjusted_mutual_info_score,\n\u001b[0;32m     11\u001b[0m     adjusted_rand_score,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m     v_measure_score,\n\u001b[0;32m     24\u001b[0m )\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_unsupervised\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     26\u001b[0m     calinski_harabasz_score,\n\u001b[0;32m     27\u001b[0m     davies_bouldin_score,\n\u001b[0;32m     28\u001b[0m     silhouette_samples,\n\u001b[0;32m     29\u001b[0m     silhouette_score,\n\u001b[0;32m     30\u001b[0m )\n\u001b[0;32m     32\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madjusted_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnormalized_mutual_info_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsensus_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     52\u001b[0m ]\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\metrics\\cluster\\_unsupervised.py:22\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _safe_indexing, check_random_state, check_X_y\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_param_validation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     Interval,\n\u001b[0;32m     19\u001b[0m     StrOptions,\n\u001b[0;32m     20\u001b[0m     validate_params,\n\u001b[0;32m     21\u001b[0m )\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _VALID_METRICS, pairwise_distances, pairwise_distances_chunked\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_number_of_labels\u001b[39m(n_labels, n_samples):\n\u001b[0;32m     26\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that number of labels are valid.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m \n\u001b[0;32m     28\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;124;03m        Number of samples.\u001b[39;00m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\metrics\\pairwise.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mparallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Parallel, delayed\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _num_samples, check_non_negative\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_distances_reduction\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArgKmin\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_pairwise_fast\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _chi2_kernel_fast, _sparse_manhattan\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Utility Functions\u001b[39;00m\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\__init__.py:94\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Pairwise Distances Reductions\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# =============================\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m#    (see :class:`MiddleTermComputer{32,64}`).\u001b[39;00m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dispatcher\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     95\u001b[0m     ArgKmin,\n\u001b[0;32m     96\u001b[0m     ArgKminClassMode,\n\u001b[0;32m     97\u001b[0m     BaseDistancesReductionDispatcher,\n\u001b[0;32m     98\u001b[0m     RadiusNeighbors,\n\u001b[0;32m     99\u001b[0m     RadiusNeighborsClassMode,\n\u001b[0;32m    100\u001b[0m     sqeuclidean_row_norms,\n\u001b[0;32m    101\u001b[0m )\n\u001b[0;32m    103\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseDistancesReductionDispatcher\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArgKmin\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean_row_norms\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    110\u001b[0m ]\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# ruff: noqa: E501\u001b[39;00m\n",
      "File \u001b[1;32mD:\\code\\cuda\\lib\\site-packages\\sklearn\\metrics\\_pairwise_distances_reduction\\_dispatcher.py:13\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_dist_metrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      9\u001b[0m     BOOL_METRICS,\n\u001b[0;32m     10\u001b[0m     METRIC_MAPPING64,\n\u001b[0;32m     11\u001b[0m     DistanceMetric,\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     14\u001b[0m     ArgKmin32,\n\u001b[0;32m     15\u001b[0m     ArgKmin64,\n\u001b[0;32m     16\u001b[0m )\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_argkmin_classmode\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     18\u001b[0m     ArgKminClassMode32,\n\u001b[0;32m     19\u001b[0m     ArgKminClassMode64,\n\u001b[0;32m     20\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _sqeuclidean_row_norms32, _sqeuclidean_row_norms64\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1002\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:945\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1439\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1411\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1544\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:147\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a743ea",
   "metadata": {
    "papermill": {
     "duration": 0.012679,
     "end_time": "2021-10-27T20:35:39.376759",
     "exception": false,
     "start_time": "2021-10-27T20:35:39.364080",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"2\"></a> <br>\n",
    "### Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63287293",
   "metadata": {
    "papermill": {
     "duration": 5.304493,
     "end_time": "2021-10-27T20:35:44.694125",
     "exception": false,
     "start_time": "2021-10-27T20:35:39.389632",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Prepare Dataset\n",
    "# load data\n",
    "train = pd.read_csv(\"../Data/mnist/train.csv\",dtype = np.float32)\n",
    "\n",
    "# split data into features(pixels) and labels(numbers from 0 to 9)\n",
    "targets_numpy = train.label.values\n",
    "features_numpy = train.loc[:,train.columns != \"label\"].values/255 # normalization\n",
    "\n",
    "# train test split. Size of train data is 80% and size of test data is 20%. \n",
    "features_train, features_test, targets_train, targets_test = train_test_split(features_numpy,\n",
    "                                                                             targets_numpy,\n",
    "                                                                             test_size = 0.2,\n",
    "                                                                             random_state = 42) \n",
    "\n",
    "# create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore first we create tensor, then we will create variable\n",
    "featuresTrain = torch.from_numpy(features_train)\n",
    "targetsTrain = torch.from_numpy(targets_train).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# create feature and targets tensor for test set.\n",
    "featuresTest = torch.from_numpy(features_test)\n",
    "targetsTest = torch.from_numpy(targets_test).type(torch.LongTensor) # data type is long\n",
    "\n",
    "# batch_size, epoch and iteration\n",
    "batch_size = 100\n",
    "n_iters = 10000\n",
    "num_epochs = n_iters / (len(features_train) / batch_size)\n",
    "num_epochs = int(num_epochs)\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(featuresTrain,targetsTrain)\n",
    "test = torch.utils.data.TensorDataset(featuresTest,targetsTest)\n",
    "\n",
    "# data loader\n",
    "train_loader = DataLoader(train, batch_size = batch_size, shuffle = False)\n",
    "test_loader = DataLoader(test, batch_size = batch_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa046da8",
   "metadata": {
    "papermill": {
     "duration": 0.011641,
     "end_time": "2021-10-27T20:35:44.718032",
     "exception": false,
     "start_time": "2021-10-27T20:35:44.706391",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"3\"></a> <br>\n",
    "### Create ANN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c557d83",
   "metadata": {
    "papermill": {
     "duration": 0.044346,
     "end_time": "2021-10-27T20:35:44.774452",
     "exception": false,
     "start_time": "2021-10-27T20:35:44.730106",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create ANN Model\n",
    "class ANNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(ANNModel, self).__init__()\n",
    "        \n",
    "        # Linear function 1: 784 --> 150\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim) \n",
    "        # Non-linearity 1\n",
    "        self.relu1 = nn.ReLU()\n",
    "        \n",
    "        # Linear function 2: 150 --> 150\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 2\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        \n",
    "        # Linear function 3: 150 --> 150\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        # Non-linearity 3\n",
    "        self.elu3 = nn.ELU()\n",
    "        \n",
    "        # Linear function 4 (readout): 150 --> 10\n",
    "        self.fc4 = nn.Linear(hidden_dim, output_dim)  \n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Linear function 1\n",
    "        out = self.fc1(x)\n",
    "        # Non-linearity 1\n",
    "        out = self.relu1(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc2(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.tanh2(out)\n",
    "        \n",
    "        # Linear function 2\n",
    "        out = self.fc3(out)\n",
    "        # Non-linearity 2\n",
    "        out = self.elu3(out)\n",
    "        \n",
    "        # Linear function 4 (readout)\n",
    "        out = self.fc4(out)\n",
    "        return out\n",
    "\n",
    "# instantiate ANN\n",
    "input_dim = 28*28\n",
    "hidden_dim = 150 #hidden layer dim is one of the hyper parameter and it should be chosen and tuned. For now I only say 150 there is no reason.\n",
    "output_dim = 10\n",
    "\n",
    "# Create ANN\n",
    "model = ANNModel(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "# Cross Entropy Loss \n",
    "error = nn.CrossEntropyLoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.02\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed1899d",
   "metadata": {
    "papermill": {
     "duration": 0.011819,
     "end_time": "2021-10-27T20:35:44.798621",
     "exception": false,
     "start_time": "2021-10-27T20:35:44.786802",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"4\"></a> <br>\n",
    "### ANN model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d03f7a",
   "metadata": {
    "papermill": {
     "duration": 55.640625,
     "end_time": "2021-10-27T20:36:40.451302",
     "exception": false,
     "start_time": "2021-10-27T20:35:44.810677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ANN model training\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "accuracy_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "\n",
    "        train = Variable(images.view(-1, 28*28))\n",
    "        labels = Variable(labels)\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward propagation\n",
    "        outputs = model(train)\n",
    "        \n",
    "        # Calculate softmax and ross entropy loss\n",
    "        loss = error(outputs, labels)\n",
    "        \n",
    "        # Calculating gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        count += 1\n",
    "        \n",
    "        if count % 50 == 0:\n",
    "            # Calculate Accuracy         \n",
    "            correct = 0\n",
    "            total = 0\n",
    "            # Predict test dataset\n",
    "            for images, labels in test_loader:\n",
    "\n",
    "                test = Variable(images.view(-1, 28*28))\n",
    "                \n",
    "                # Forward propagation\n",
    "                outputs = model(test)\n",
    "                \n",
    "                # Get predictions from the maximum value\n",
    "                predicted = torch.max(outputs.data, 1)[1]\n",
    "                \n",
    "                # Total number of labels\n",
    "                total += len(labels)\n",
    "\n",
    "                # Total correct predictions\n",
    "                correct += (predicted == labels).sum()\n",
    "            \n",
    "            accuracy = 100 * correct / float(total)\n",
    "            \n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.data)\n",
    "            iteration_list.append(count)\n",
    "            accuracy_list.append(accuracy)\n",
    "        if count % 500 == 0:\n",
    "            # Print Loss\n",
    "            print('Iteration: {}  Loss: {}  Accuracy: {} %'.format(count, loss.data, accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f724d819",
   "metadata": {
    "papermill": {
     "duration": 0.018588,
     "end_time": "2021-10-27T20:36:40.488827",
     "exception": false,
     "start_time": "2021-10-27T20:36:40.470239",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"5\"></a> <br>\n",
    "### visualization loss "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93c2e32d",
   "metadata": {
    "papermill": {
     "duration": 0.246783,
     "end_time": "2021-10-27T20:36:40.754030",
     "exception": false,
     "start_time": "2021-10-27T20:36:40.507247",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualization loss \n",
    "plt.plot(iteration_list,loss_list)\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"ANN: Loss vs Number of iteration\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "389a6001",
   "metadata": {
    "papermill": {
     "duration": 0.019115,
     "end_time": "2021-10-27T20:36:40.792607",
     "exception": false,
     "start_time": "2021-10-27T20:36:40.773492",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"6\"></a> <br>\n",
    "### visualization accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371e615b",
   "metadata": {
    "papermill": {
     "duration": 0.26236,
     "end_time": "2021-10-27T20:36:41.074336",
     "exception": false,
     "start_time": "2021-10-27T20:36:40.811976",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# visualization accuracy \n",
    "plt.plot(iteration_list,accuracy_list,color = \"red\")\n",
    "plt.xlabel(\"Number of iteration\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"ANN: Accuracy vs Number of iteration\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 74.115706,
   "end_time": "2021-10-27T20:36:41.909032",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-10-27T20:35:27.793326",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
